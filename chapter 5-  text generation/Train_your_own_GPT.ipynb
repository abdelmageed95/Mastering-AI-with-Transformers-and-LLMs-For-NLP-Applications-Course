{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzMCUov9n5uE",
        "outputId": "9f695d7b-4108-41e7-bfe1-e78ab5d72877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-07 20:50:50--  https://www.gutenberg.org/cache/epub/18857/pg18857.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 518044 (506K) [text/plain]\n",
            "Saving to: ‘pg18857.txt’\n",
            "\n",
            "pg18857.txt         100%[===================>] 505.90K  1.44MB/s    in 0.3s    \n",
            "\n",
            "2023-09-07 20:50:55 (1.44 MB/s) - ‘pg18857.txt’ saved [518044/518044]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Journey to the Centre of the Earth\n",
        "! wget https://www.gutenberg.org/cache/epub/18857/pg18857.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzGaxY0ooWZQ",
        "outputId": "67c64d31-be27-48c2-ed97-5f5da6913760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<0.17,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface_hub, transformers\n",
            "Successfully installed huggingface_hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"
          ]
        }
      ],
      "source": [
        "! pip install tokenizers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J35WZFWxoCnn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tokenizers import models , Tokenizer, decoders , pre_tokenizers , trainers , normalizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ftf-eH45odyI"
      },
      "outputs": [],
      "source": [
        "# we will make the tokenizer object from the BPE tokenizer class\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "\n",
        "# For the normalization part we will add the Lowercase function\n",
        "tokenizer.normalizer = normalizers.Sequence([normalizers.Lowercase()])\n",
        "\n",
        "# the pre_tokenizer attribute is set to be as ByteLevel to ensure we have bytes as our input\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "\n",
        "# The decoder attribute must be also set to ByteLevelDecoder to be able to decode correctly\n",
        "tokenizer.decoder = decoders.ByteLevel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w5AXFjNxom1Z"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=50000,\n",
        "                     inital_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
        "                     special_tokens=[\n",
        "                                      \"<s>\",\n",
        "                                      \"<pad>\",\n",
        "                                      \"</s>\",\n",
        "                                      \"<unk>\",\n",
        "                                      \"<mask>\"])\n",
        "tokenizer.train([\"/content/pg18857.txt\"], trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sxv2mV81ouIQ"
      },
      "outputs": [],
      "source": [
        "! mkdir our_GPT_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ojo0V9t2ousF"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"our_GPT_tokenizer/tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a2oCyTrfo_Uo"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2Config, TFGPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeRy2cRrpMYt",
        "outputId": "9a9d0397-5c4f-4271-ee2c-21c4475a94d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "our_tokenizer = GPT2TokenizerFast.from_pretrained(\"our_GPT_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X9L1rrmpXFy",
        "outputId": "bf4a7c4b-949e-46d8-9e5e-6c860d0fd6ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "our_tokenizer.add_special_tokens({\n",
        "        \"eos_token\": \"</s>\",\n",
        "        \"bos_token\": \"<s>\",\n",
        "        \"unk_token\": \"<unk>\",\n",
        "        \"pad_token\": \"<pad>\",\n",
        "        \"mask_token\": \"<mask>\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gioB5inGpdDU",
        "outputId": "742f3e65-8d6c-434b-b845-cadc9fbaf3d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "our_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHCsr81upf6m",
        "outputId": "3332e877-32ef-4711-ad96-bb6e106cd934"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [0, 148, 118, 51, 83, 45, 91, 111, 1696, 72, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "en = our_tokenizer(\"<s> hello AI Mastery </s>\")\n",
        "en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5d9VOQTFrJv",
        "outputId": "99f11195-8286-43e4-bfb8-092ef6ccc1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'Ġhe', 'll', 'o', 'Ġa', 'i', 'Ġm', 'as', 'tery', 'Ġ', '</s>']\n"
          ]
        }
      ],
      "source": [
        "tokens = our_tokenizer.convert_ids_to_tokens(en.input_ids)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGJV-1Wvv_sI",
        "outputId": "391bab84-6fbe-4ed3-ba2b-e45816562d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tokenizers.normalizers.Sequence object at 0x7db85aa764f0>\n"
          ]
        }
      ],
      "source": [
        "print(our_tokenizer.backend_tokenizer.normalizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVCKvFZm3RRg",
        "outputId": "34ac09de-390f-4e46-f84f-d2af62ce43ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<', (0, 1)), ('s', (1, 2)), ('>', (2, 3)), ('Ġhi', (3, 6)), ('Ġthere', (6, 12)), ('Ġ</', (12, 15)), ('s', (15, 16)), ('>', (16, 17))]\n"
          ]
        }
      ],
      "source": [
        "# Let’s now take a look at the pretokenization:\n",
        "print(our_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"<s> hi there </s>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rvDEStskpncH"
      },
      "outputs": [],
      "source": [
        "config = GPT2Config(\n",
        "  vocab_size=our_tokenizer.vocab_size,\n",
        "  bos_token_id=our_tokenizer.bos_token_id,\n",
        "  eos_token_id=our_tokenizer.eos_token_id)\n",
        "\n",
        "model = TFGPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubVa8Fxjp9CQ",
        "outputId": "890c2399-ad1d-4b6d-8c87-083800997465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.33.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 13343\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vPE4Wjr5t4K9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SogeqQdQqGsh"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/pg18857.txt\", \"r\", encoding='utf-8') as f:\n",
        "    content = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content[156]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XjiQIeIudNi6",
        "outputId": "294a2067-43f5-42c8-ba88-de21c1324061"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'truly so wonderful that even now I am bewildered when I think of them.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "NLeEH-87qGqT"
      },
      "outputs": [],
      "source": [
        "content_p = []\n",
        "for c in content:\n",
        "    if len(c)>10:\n",
        "        content_p.append(c.strip() )\n",
        "\n",
        "content_p = \" \".join(content_p) + our_tokenizer.eos_token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content_p[1000:1300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "AQti5nK0dyYf",
        "outputId": "6a7626a5-e38c-4923-cffe-6ab63cafe19c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' of Jules Verne. First published in England by Griffith and Farran, 1871, this edition is not a translation at all but a complete re-write of the novel, with portions added and omitted, and names changed. The most reprinted version, it is entered into Project Gutenberg for reference purposes only. A'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-O-49JKUqGno"
      },
      "outputs": [],
      "source": [
        "tokenized_content = our_tokenizer.encode(content_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efj2voxIqGlQ",
        "outputId": "090fbd3e-54b9-49af-ec18-75008e1e5c9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104664"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "len(tokenized_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "eI8owH7qqGio"
      },
      "outputs": [],
      "source": [
        "sample_len = 100\n",
        "examples = []\n",
        "for i in range(0, len(tokenized_content)):\n",
        "    examples.append(tokenized_content[i:i + sample_len])\n",
        "\n",
        "train_data = []\n",
        "labels = []\n",
        "for example in examples:\n",
        "    train_data.append(example[:-1])  # 0 -->  99\n",
        "    labels.append(example[1:])    #    1 --> 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjIoPYRkiFOg",
        "outputId": "a1868a10-f968-49bd-94b4-eed985b0399c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104664"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LNfTAPEdqGeA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "buffer = 100\n",
        "batch_size = 16\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_data[:100000],labels[:100000]))\n",
        "dataset = dataset.shuffle(buffer).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsNXzVRuraU4",
        "outputId": "95d9b8cc-97f1-4904-ab57-4c8fb332dc63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(16, 99), dtype=tf.int32, name=None), TensorSpec(shape=(16, 99), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "416mOJO7qx-7"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L7ajDbRq4PU",
        "outputId": "27a576e0-4a0f-48fd-a0ba-b6a33129bbff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "6250/6250 [==============================] - 2576s 406ms/step - loss: 5.1409 - accuracy: 0.1845\n",
            "Epoch 2/5\n",
            "6250/6250 [==============================] - 2542s 407ms/step - loss: 4.4122 - accuracy: 0.2246\n",
            "Epoch 3/5\n",
            "6250/6250 [==============================] - 2542s 407ms/step - loss: 3.7538 - accuracy: 0.2836\n",
            "Epoch 4/5\n",
            "6250/6250 [==============================] - 2543s 407ms/step - loss: 2.8431 - accuracy: 0.4025\n",
            "Epoch 5/5\n",
            "6250/6250 [==============================] - 2541s 407ms/step - loss: 1.6049 - accuracy: 0.6362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7db7ead47fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "epochs = 5\n",
        "model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "bf8Pfx6Bq4AY"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"Mini_GPT2_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "zvlyWFA22MCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1118c65-2d1c-4292-d0d4-7e43829aa442"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Mini_GPT2_tok/tokenizer_config.json',\n",
              " 'Mini_GPT2_tok/special_tokens_map.json',\n",
              " 'Mini_GPT2_tok/vocab.json',\n",
              " 'Mini_GPT2_tok/merges.txt',\n",
              " 'Mini_GPT2_tok/added_tokens.json',\n",
              " 'Mini_GPT2_tok/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from transformers import WEIGHTS_NAME, CONFIG_NAME, TF2_WEIGHTS_NAME\n",
        "our_tokenizer.save_pretrained(\"Mini_GPT2_tok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ucm-OTBE2QFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a5b464-733d-46cf-aae1-f4c86e2b7404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at Mini_GPT2_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"Mini_GPT2_tok\")\n",
        "model_reloaded = TFGPT2LMHeadModel.from_pretrained(\"Mini_GPT2_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ql-HJJyf2Vxc"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, model):\n",
        "    input_token_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    output = model.generate(\n",
        "        input_token_ids,\n",
        "        pad_token_id = tokenizer.pad_token_id,\n",
        "        eos_token_id = tokenizer.eos_token_id,\n",
        "        max_length = 128,\n",
        "        num_beams = 5,\n",
        "        temperature = 0.7,\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WluypVOu2gEj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "ca57b641-9858-4300-9dcd-04a8331b3e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is an exciting idea  chorus. we have reached the superstitious character of san vicenza, where hans demanded the price of his thirteenth week of service. my uncle paid him, with very many warm shakes of the hand. at that moment, if he did not indeed quite share our natural emotion, he allowed his feelings so far to give way as to indulge in an extraordinary expression for him. with the tips of two fingers he gently pressed our hands and smiled. chapter 44 the journey ended this is the final conclusion of a narrative which will be probably disbelieved even by people who are astonished at nothing. i am, however, i do not'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "generate(\"it is an exciting idea \" , model_reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dfj8CaSmVcco"
      },
      "execution_count": 70,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}