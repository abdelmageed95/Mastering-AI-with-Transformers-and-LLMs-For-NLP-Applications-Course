# Mastering AI with Transformers and LLMs For NLP Applications Course

**Mastering AI With Transformers and LLMs for NLP Applications isn't just a course;** it's a transformative experience that arms learners with the expertise, practical skills, and innovation-driven mindset needed to navigate and lead in the ever-evolving landscape of Artificial Intelligence.

<img src="photo1699818379 (1).jpeg">

## Course Overview

Embark on a transformative journey into the heart of Large Language Models (LLMs) with our comprehensive course. Dive deep into the intricacies of Transformers, unraveling their magic layer by layer, from the basics to building your own models and deploying them in production.

### Course Content

#### Chapter 1: Introduction (Understanding Transformers)
1. Explore Transformer's Pipeline Module - [Notbook](chapter%201%20introduction/introduction.ipynb)
1. High-Level Understanding of Transformers Architecture
2. What are Language Models

#### Chapter 2: Transformers Architecture
1. Input Embedding - [Notebook](chapter%202%20Transformers%20Architecture/1-Input%20Embedding.ipynb)
2. Positional Encoding
3. The Encoder
4. The Decoder
5. Autoencoding LM - BERT
6. Autoregressive LM - GPT
7. Sequence2Sequence LM - T5
8. Tokenization - [Notebook](chapter%202%20Transformers%20Architecture/8-Tokenization(optional).ipynb)

#### Chapter 3: Text Classification
1. Fine-tuning BERT for Sentiment Analysis - [Notebook](chapter%203-%20text%20classification/1-fine_tune_bert_for_sentiment_analysis.ipynb)
2. Fine-tuning BERT for Multi-Class Classification - [Notebook](chapter%203-%20text%20classification/2-Fine_tuning_BERT_for_multi_class_classifiation.ipynb)
3. Fine-tuning BERT for Sentence-Pairs - [Notebook](chapter%203-%20text%20classification/3-%20Fine-tuning%20the%20BERT%20model%20for%20sentence-pair.ipynb)

#### Chapter 4: Question Answering
1. QA Intuition
2. Build a QA System based on Amazon Reviews - [Notebook](chapter%204-%20QA/2_Amazon_Reviews_QA_.ipynb)
3. Implement Retriever Reader Approach - [Notebook](chapter%204-%20QA/3-Retriever%20Reader%20Approach.ipynb)
4. Fine Tuning Transformers for Question Answering Systems - [Notebook](chapter%204-%20QA/4-%20fine%20tune%2-QA.ipynb)
5. Table QA - [Notebook](chapter%204-%20QA/5-%20Table_QA.ipynb)

#### Chapter 5: Text Generation
1. Introduction to Text Generation
1. Greedy Search Decoding - [Notebook](chapter%205-%20%20text%20generation/2-%20Greedy%20Search%20Decoding.ipynb)
2. Beam Search Decoding - [Notebook](chapter%205-%20%20text%20generation/3_beam_search_decoding.ipynb)
3. Sampling Methods - [Notebook](chapter%205-%20%20text%20generation/4_sampling_methods.ipynb)
4. Train Your Own GPT - [Notebook](chapter%205-%20%20text%20generation/Train_your_own_GPT.ipynb)

#### Chapter 6: Text Summarization
1. Introduction to GPT2, T5, BART, PEGASUS
2. Evaluation Metrics - Bleu Score, ROUGE
3. Fine-Tuning PEGASUS for Dialogue Summarization - [Notebook](chapter%206%20-text%20summarization/Summarization.ipynb)

#### Chapter 7: Build Your Own Transformer From Scratch
1. Build Custom Tokenizer - [Notebook](chapter%207%20-%20-Transformer%20from%20scratch/2_build_tokenizer_russian.ipynb)
2. Getting Your Data Ready - [Notebook](chapter%207%20-%20-Transformer%20from%20scratch/implement_transformer_layers.ipynb)
3. Implement Positional Embedding - [Notebook](chapter%207%20-%20-Transformer%20from%20scratch/implement_transformer_layers.ipynb)
4. Implement Transformer Architecture - [Notebook](chapter%207%20-%20-Transformer%20from%20scratch/implement_transformer_layers.ipynb)

#### Chapter 8: Deploy Transformers Model in Production Environment
1. Model Optimization with Knowledge Distillation - [Notebook](chapter%208%20Transformer%20in%20production/Model_Optimization_.ipynb)
2. Model Optimization with Quantization - [Notebook](chapter%208%20Transformer%20in%20production/Model_Optimization_.ipynb)
3. Model Optimization with ONNX and the ONNX Runtime - [Notebook](chapter%208%20Transformer%20in%20production/Model_Optimization_.ipynb)
4. Serving Transformers with Fast API, Dockerizing Your Transformers APIs - [Project](chapter%208%20Transformer%20in%20production/app)

### Requirements
- Basic Python Programming
- Fundamental Machine Learning Knowledge
- NLP Basics (Optional)
- Python and Jupyter Notebooks

### Why Enroll?
- Comprehensive Learning: From theory to practical application and deployment.
- Practical Knowledge: Transform theoretical concepts into practical skills.
- Versatility: Harness Transformer powers for diverse tasks.
- Innovation: Become the architect of AI innovation.
- Deployment Mastery: Seamlessly deploy models in production environments.
- Real-World Relevance: Skills directly applicable to real-world scenarios.

### Target Audience
- AI Enthusiasts and Beginners
- Data Scientists and Machine Learning Engineers
- Developers and Programmers
- NLP Enthusiasts
- Researchers and Academics
- AI Innovators and Entrepreneurs

### Coming Soon!
- AI For Audios Course
- Transformers for Vision Course
